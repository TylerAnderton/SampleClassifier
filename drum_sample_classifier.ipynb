{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drum Sample Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to develop a pipeline to process user-provided drum samples and apply the model's classification prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define test filenames & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KICK_1_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/KICKS/ELECTRONIC/Antidote Audio', 'Antidote - Kick 2.wav')\n",
    "KICK_2_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/KICKS/ELECTRONIC/Nitti Gritti', '808 Top Kick.wav')\n",
    "\n",
    "SNARE_1_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/SNARES/ACOUSTIC/Apashe', 'Apashe_Acoustic_Snare.wav')\n",
    "SNARE_2_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/SNARES/TRAP/Kompany', 'Kompany - Snare 4.wav')\n",
    "\n",
    "PERC_1_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/PERCS/BLOCKS, RIMS, ETC/Cymatics', 'Cymatics - 100k Perc 2.wav')\n",
    "PERC_2_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/TOMS/ELECTRONIC/PhaseOne', 'PhaseOne_Tom1.wav')\n",
    "\n",
    "CYMBAL_1_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/CYMBALS/CRASHES/ELECTRONIC/Au5', 'Au5_cymbal_crash_acoustic.wav')\n",
    "CYMBAL_2_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/CYMBALS/CRASHES/ELECTRONIC/UpSound', 'Crash 13.wav')\n",
    "CYMBAL_3_FILEPATH: str = os.path.join('/Users/tyler/2TB SSD/Samples/ALL/ONE SHOTS/DRUMS/CYMBALS/RIDES/ACOUSTIC/Cymatics', 'Cymatics - Ride 1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE: int = 44100\n",
    "SAMPLE_LENGTH: int = 132300\n",
    "\n",
    "FRAME_LENGTH: int = 2**10\n",
    "N_BINS: int = int(FRAME_LENGTH / 2 + 1)\n",
    "FRAME_STEP: int = int(FRAME_LENGTH / 8)\n",
    "N_FRAMES: int = int(((SAMPLE_LENGTH - FRAME_LENGTH) / FRAME_STEP) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load & preprocess sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building & training the model, I had to be careful about only using TensorFlow modules that would map to the Dataset properly. The tf.audio.decode_wav() module only supports 16bit audio, which required an extra step of writing new 16bit files from the 24bit originals. Since I am not mapping these transormations here to an entire dataset, I should be fine using 24bit audio samples with the librosa library. Ultimately, I am converting the audio to STFT spectrograms before passing to the classification model, so specific bit depth shouldn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(filepath: str) -> tf.Tensor:\n",
    "    audio, sample_rate = librosa.load(filepath, mono=True, sr=SAMPLE_RATE)\n",
    "    return tf.convert_to_tensor(audio, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pad & trim sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sample(audio: tf.Tensor) -> tf.Tensor:\n",
    "    audio = audio[:SAMPLE_LENGTH]\n",
    "\n",
    "    zero_padding = tf.zeros([SAMPLE_LENGTH] - tf.shape(audio), dtype=tf.float32)\n",
    "    return tf.concat([audio, zero_padding], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Normalize sample volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(audio: tf.Tensor) -> tf.Tensor:\n",
    "    audio_max = tf.reduce_max(tf.abs(audio))\n",
    "    scale_factor = 1 / audio_max\n",
    "    return audio * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kick_1_norm = normalize(pad_sample(load_sample(KICK_1_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(132300,), dtype=float32, numpy=\n",
       "array([ 0.09780365, -0.06092693, -0.43853027, ...,  0.        ,\n",
       "        0.        ,  0.        ], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kick_1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.16108>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(kick_1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(tf.abs(kick_1_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Apply STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stft(audio: tf.Tensor) -> tf.Tensor:\n",
    "    spectrogram = tf.signal.stft(audio, frame_length=FRAME_LENGTH, frame_step=FRAME_STEP) \n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2) #convolution neural net expects channels dimension\n",
    "    return tf.expand_dims(spectrogram, axis=0) #model also expects batch size dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Wrap processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process(filepath: str) -> tf.Tensor:\n",
    "    return apply_stft(normalize(pad_sample(load_sample(filepath))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1026, 513, 1), dtype=float32, numpy=\n",
       "array([[[[1.0555505e+00],\n",
       "         [9.8869829e+00],\n",
       "         [4.0773315e+01],\n",
       "         ...,\n",
       "         [2.9656422e-01],\n",
       "         [3.0780557e-01],\n",
       "         [3.1764221e-01]],\n",
       "\n",
       "        [[6.9636750e+00],\n",
       "         [1.7453072e+01],\n",
       "         [6.6924355e+01],\n",
       "         ...,\n",
       "         [1.0898353e-01],\n",
       "         [1.7584307e-01],\n",
       "         [2.6164484e-01]],\n",
       "\n",
       "        [[1.3303730e+00],\n",
       "         [2.4632553e+01],\n",
       "         [1.0118411e+02],\n",
       "         ...,\n",
       "         [3.8304087e-02],\n",
       "         [7.3528975e-02],\n",
       "         [1.3738275e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         ...,\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00]],\n",
       "\n",
       "        [[0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         ...,\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00]],\n",
       "\n",
       "        [[0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         ...,\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00],\n",
       "         [0.0000000e+00]]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kick_1_test_stft = load_and_process(KICK_1_FILEPATH)\n",
    "kick_1_test_stft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load saved model and make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_training/trained_hypermodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 12:43:02.984227: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-02-25 12:43:03.040764: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.8817014e-18, 1.0000000e+00, 1.2962743e-10, 5.6914194e-21]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kick_1_pred: list[list[float]] = model.predict(load_and_process(KICK_1_FILEPATH))\n",
    "kick_1_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Translate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_prediction(prediction_array: list[float]) -> str:\n",
    "    if np.argmax(prediction_array) == 0:\n",
    "        return 'cymbal'\n",
    "    elif np.argmax(prediction_array) == 1:\n",
    "        return 'kick'\n",
    "    elif np.argmax(prediction_array) == 2:\n",
    "        return 'perc'\n",
    "    elif np.argmax(prediction_array) == 3:\n",
    "        return 'snare'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test predictions on all test samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the tests below, the model is not perfect. We likely could have achieved greater accuracy with a larger and more diverse training dataset.\n",
    "\n",
    "However even though the model isn't perfect, out of the 20 tests below, the model accurately predicted the label 17 times, perfectly reflecting the training val_accuracy of 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kick'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(KICK_1_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kick'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(KICK_2_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'snare'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(SNARE_1_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'snare'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(SNARE_2_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'snare'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(PERC_1_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'perc'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(PERC_2_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(CYMBAL_1_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'perc'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(CYMBAL_2_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process(CYMBAL_3_FILEPATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/cymbal_001111.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/cymbal_001986.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/cymbal_001992.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/cymbal_002741.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/cymbal_009212.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kick'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/kick_015418.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kick'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/kick_016314.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cymbal'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/perc_019687.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'perc'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/perc_019681.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'snare'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/snare_020045.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'snare'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prediction(model.predict(load_and_process('model_training/preprocessed_samples_16bit/snare_020044.wav')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "87c1d7f5e1ccccee12e5578f273ffc9389e1f93823c720f1af1a96a1bb04cc3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
